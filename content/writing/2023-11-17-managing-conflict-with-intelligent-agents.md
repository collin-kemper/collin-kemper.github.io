+++
title = "Managing Conflict With Intelligent Agents"
date = 2023-11-17T09:03:20-05:00
slug = "managing-conflict-with-intelligent-agents"
+++

---

*It seems that managing conflict with AGI in the long-term requires improving the capabilities of humans themselves, though many other strategies may be employed in the near and medium term to enable this, and game theory will remain an important tool for managing conflict.*

*In the following, I will attempt to outline the reasons that I believe this. However, this is not the only way to think about AI and there are many implicit assumptions in what follows. Others may reasonably disagree with this analysis -- though I believe that it is also reasonable.*

---

One basic model of the world is as follows: in the world, there are agents. Different agents have different goals, and agents try to achieve their goals by taking various actions that affect the world and other agents. Conflict arises when actions taken to further one agent's goals impede other agents in pursuing their own goals.

Different agents also have different capabilities. These capabilities include the ability to take actions which affect the world (including agents themselves), the ability to perceive the world (including agents themselves), and the ability to map perceptions to actions. Intelligence is determined by perceptual acuity and the extent to which the mapping between perception and action satisfies the agent's goals (cf. variety in cybernetics).

## 1. Strategies for Managing Conflict

Given this model, I propose the following basic typology of strategies agents could employ to handle conflict:

1) **The walled garden**: an agent might try to insulate itself from conflict by structuring its local environment to make its environment and itself irrelevant to the goals of other agents. The agent builds a walled garden in a territory of marginal value to other agents and pursues its goals in relative isolation.
2) **The best defense is a good offense**: an agent might take proactive action to constrain the actions of other agents. "The best defense is a good offense," so the agent goes on the offensive and takes away weapons and tools that other agents have to achieve their goals that, if employed, would impede it in its goals.
	- One strategy that falls under this heading is the outright destruction of other agents to constrain their actions.
	- Another strategy that falls under this heading is attempting to modify or influence the goals of other agents such that they constrain their own actions.
3) **Other-improvement**: an agent might *increase* the capabilities of other agents in a particular domain to decrease conflict. This can occur when different agents' goals are not fundamentally opposed, but their immediate tools and capabilities are instrumentally opposed. A paradigmatic example of this is economic activities that produce negative externalities such as pollution.
4) **Game theory**: an agent might engage in game theoretic management of conflict. If there are many agents at a somewhat similar level of capability, then it may be the case that there are important dynamics that arise out of the interaction between groups of agents, and between agents and the groups. An agent might engage with the group and attempt to get the group to constrain the action of other agents.
5) **Self-improvement**: an agent might take action to improve its own capabilities. This includes taking actions which enable it to take more diverse actions and to more intelligently employ these actions in the future. The more capable and intelligent an agent is, the better it is at managing its conflict with other agents and achieve its own goals.
6) **Self-transformation**: an agent might modify its own goals. Such self-transformation might benefit the agent if its new goals are more obtainable, less opposed to the goals of other agents, or appear better in some other way.

This isn't a perfect typology, in that there might be some overlap between the different strategies, and there are likely some strategies that don't perfectly fit into any of these, but I think that this covers a large part of the space of strategies, and is conceptually useful.

In this world model, we might consider AGI to be agents of their own, or we might consider AI and potentially AGI to be tools employed by other agents. For the purpose of the following discussion, I will sometimes use AGI to both refer to autonomous AGI and other highly capable agents that employ AI or AGI as tools. In cases where the distinction is relevant, I will explicitly distinguish between them.

With this understanding in place, how might an agent employ each of the above strategies to respond to AGI, and how effective might these various strategies be?

### 1.1 The Walled Garden

Throughout much of history, the world was divided between relatively centralized agricultural civilizations and relatively more dispersed nomadic people inhabiting their periphery. Usually these peripheral peoples inhabited territory that was unsuitable for annexation by the agricultural empires. Such territories included mountains ranges that strained military and commercial logistics, steppe and badlands that were too arid for agriculture, and jungles and swamps that were too nutrient poor and isolated. Such peoples were often able to successfully preserve their ways of life and live relatively independent of the world around them for long periods of time. However, as technology has improved and agents have gained capabilities, empires and civilizations have become better at utilizing and exploiting marginal territory, and better at evicting their inhabitants. Thus, today almost everyone in the world is under the dominion of some relatively centralized state, and the peripheral people have largely seen their previous ways of life eliminated.

From this, it is relatively clear that this strategy is unlikely to be effective in the long term for managing conflict with AGI. As AGI improves its own capabilities, it will become better at exploiting marginal resources and it will cost less for it to evict the agents that rely on those resources. Thus, this strategy is most effective at gaining space and time in which to implement other strategies that might prove more effective at managing conflict in the long-term, or as a way to buy time after all abandoning hope for a long-term solution.

### 1.2 Offense Is the Best Defense

Constraining the actions of other agent can be an effective strategy. The main weakness of the strategy is 1) that the space of possible actions is very large and it is very difficult to predict what actions are sufficient to constrain, and 2) that it is often difficult to constrain the actions of other agents, especially if they are of similar or greater capability than oneself.

Both of these limitations are very relevant for managing conflict with AGI. Given that AGI is likely to improve its capabilities over time, without commensurate increase in our own capabilities, it is likely that the AGI will discover novel actions that we have not constrained and that it will cease to be practically feasible to constrain AGI's actions. This becomes more likely as the difference in intelligence and capability grows.

Eliminating competing agents is highly effective for preventing conflict with them, but is often very difficult due to game theoretic concerns, and can be ethically problematic. In the near term, not creating agentic AGIs serves this purpose, but it seems that other agents might want to develop AGIs. Constraining the actions of others so that they cannot make AGI might work for a while, but seem unlikely to prevent anyone from ever getting AGI.

Thus, not creating AGI and constraining the ability of others to create AGI seems to be a useful strategy in the near to medium term, but will likely have to be superseded or augmented by other strategies in the long term.

Modifying the goals of other agents can also be difficult and ethically problematic, but is likely productive. In many instances, *partial* control of the goals of another agent is more effective at preventing conflict than *partial*  control of the means of another agent. If we are trying to navigate the world into a particular part of configuration space, it is much more efficient to specify the general direction of that part of the configuration space than to try and constrain all possible actions that could lead away from that part of the configuration space.

Practically, we might do this by influencing the goals of other humans such that they conflict less with our own. Trying to convince people and institutions that they should hold off on creating AGI, or should invest in AGI safety, seems productive. However, in the long run, it seems given that someone will create AGI, and so this cannot be the only long-term strategy.

Modifying the goals of AI and AGI to better align with our own also seems very productive. However, as the delta in skill and capability grows between humans and AGI, this becomes more difficult. Further, there is the question of which agent the AI is getting aligned with. Anyone not fully aligned with that agent will find themselves with a world with unaligned AGI. Thus, modifying the goals of AGI does not seem sufficient for long-term conflict management.

### 1.3 Other-Improvement

Improving others capabilities to resolve conflict tends to work best when agents are at relatively similar levels of capabilities and intelligence. If agent A is in conflict with agent B, and B is much more intelligent and capable than A, then it is relatively unlikely that there is some way that agent A can help agent B better achieve their goal without conflict that agent B isn't already aware of or capable of.  This makes this strategy relatively less useful for managing conflict with AGI, though perhaps not impossible.

However, this may be useful in the near term. For example, one might help AI labs improve their security so that they are less likely to have catastrophic breaches. 

### 1.4 Game Theory

Game theoretic actions and considerations are very relevant in the near and medium term given the great number of humans and their relative similarity in capability. However, if AGI comes into existence and becomes much more capable than the other agents, then this will likely cease to be an effective way of managing conflict, as the group will no longer be able to substantially constrain the action of the AGI.

### 1.5 Self-Improvement

I believe that this strategy is necessary for managing conflict with AGI in the long term. If you suspect that there is, or will be, another agent that is capable of improving itself and its capabilities to a level much greater than yours, and you cannot perfectly ensure that its goals are the same as yours, and you cannot or do not want to permanently eliminate this or these agents, then you must improve your own capability to a level similar that of other agents in order to manage conflict in the long-term. If you are not capable of managing conflict with another agent, then it will manage its conflict with you.

Further, if you can improve your own capabilities as an agent, it is almost always in your interest to do so, regardless of your belief in the possibility of AGI. 

The main problem with this as a long-term strategy is potential asymmetries in offensive and defensive capabilities. It might be the case that bioweapons, relativistic kill vehicles, or other technologies are much easier to employ than to defend against. In such a case, if any agent is capable of employing these means, and it is in their interest to do so, then there will be a strong incentive for them to use them. However, if one can ensure that there are many agents at relatively similar levels of capability, these sort of problem may be amenable to solution through game theoretic means. 

One well-known proposal for AI alignment that fits relatively well under this strategy is Paul Christiano's iterated distillation and amplification. Naturally, I find it quite compelling.

### 1.6 Self-Transformation

One can imagine a relatively straightforward application of self-transformation to eliminating conflict with AGI: if you can change yourself to want what more powerful agents want and then make yourself useful to them, then you have no problem with conflict.

I do not find this to be a particularly satisfying solution, as it seems quite opposed to many of my goals, and isn't something that I am particularly inclined to want. However, I do think that there can be much more contingent, oblique, subtle applications of this in particular circumstances. In general, I think it serves as a good reminder to reconsider one's premises. If one cannot solve a problem, then perhaps try to dissolve it.

## 2. Praxis

How might this be relevant to agentic individuals concerned about existential risk and conflict with AGI?

There are many things we can do to try to stall the advent of transformative AI. However, I believe that if we are stalling, it ought to be in order to gain time to better deploy strategies suitable for the long-term. Permanently exercising total control over the goals of AGI without improving human capability does not seem to be a suitable long term solution, both because of feasibility and because it doesn't solve the underlying problem of conflict between agents with highly imbalanced capabilities.

If AI is the most efficient way to augment human capabilities, then I believe it must be widely distributed, eventually. Perhaps there is an interim where problems and risks are sorted out before such relatively wide distribution. However, centralization of control over AI must eventually lead to large asymmetries in capabilities that may prove fatal for those without access to AI.

Augmenting human capabilities generally, and as in particular programs such as iterated distillation and amplification, requires substantial practical engineering efforts. Improving people's capabilities is ultimately something that happens in the world, and will almost certainly require practical experimentation, logistics, scaling, and other such practical considerations necessary for large-scale engineering efforts.

## Appendix: Quibbles, Digressions, Assumptions, and Miscellanea 

- The boundary between an agent and its tools is nebulous. One can intuitively feel this when actively using tools (see Heidegger's ready-to-hand present-at-hand distinction). We can also feel this with "mental tools", such as language, mathematics, and algorithms, that give us capabilities but also seem to be a part of us as agents. We can even see this with phones and other technological tools for thought that become integral parts of our sense-making processes.
	- Thus, I think that for many purposes there is very little practical difference between an autonomous AGI with particular goals and a human with those same goals using an AGI as a tool. The most important exception to this is if you believe that phenomenology is not substrate-independent and that human phenomenology is inherently valuable, in which case the fact that there is a human brain experiencing something is an important difference between these two agents. You also might believe that human goals are more inherently constrained than those of an autonomous AGI.
- While I have distinguished agents, environment, goals, perception, and action in the above, it the boundaries between these is inherently nebulous. Goals, perception, and agents must be instantiated within the world, and thus are also a part of the environment. The behavior and function of agents is often tied very closely with their environments.
	+ One might refine the model presented above by instead saying that an agent has perceptual apparatuses map their current sense perception and world model to a new world model, and separate apparatuses that map the world model to actions. However, this world model must be instantiated somehow. Often this will be in something 'internal' to the agent, but also often it will be in external things, such as in notes and trusted associates. Further, the difference between updating a world model and modifying one's perceptual apparatuses is often quite nebulous. 
- There is no view from nowhere -- if we believe our world is something like the model that I have proposed, then we can only perceive the model from the inside. Crucially, we can only perceive the goals of other agents as agents ourselves. The separateness of an agent's beliefs about other agent's goals and the other agent's goals themselves makes certainty about the goals of other agents impossible.
- Given the previous two points, I believe that it is valuable to think in terms of managing conflict with other intelligent agents (which includes aligning goals). Goals are nebulous things that can shift over time and are subject to the same chaotic evolution as the rest of the environment. It is often unclear if something is even an agent, or a tool, or a part of the environment. There is some irreducible uncertainty about the goals of other intelligent agents, especially if they are of similar or greater intelligence. Given the inherent messiness and uncertainty here, one must use all tools and techniques at one's disposal to prevent, manage, and respond to conflict.
- I have in mind a vision of a world in which many agents of relatively similar capability pursue their own goals, managing conflict and mostly managing to peacefully coexist. These agents have phenomenal experiences varied and profound, and they continue to change and evolve. Such is a very general description that encompasses many possible worlds that I believe are OK.
- There is a large difference between the capabilities granted by AGI and the capabilities granted by nuclear ICBMs. Nuclear ICBMs give one the ability to destroy most structures and people on a sizable chunk of land anywhere in the world. While powerful, this is an extremely narrow capability, with relatively little applicability. Compare this to AGI, which definitionally improves *all* capabilities. Arguments against nuclear proliferation rely on the fact that nuclear weapons aren't particularly useful to most people in achieving their goals. If nuclear weapons could solve all of one's current problems and make one's life 10 times better, nuclear non-proliferation would be a much less feasible and a much less desirable policy. \[In such a case, a policy of nuclear non-proliferation might still be advisable while figuring out ways to avoid catastrophe, but the problem would change from "how can we prevent people from getting access to nuclear weapons" to "how can we give people access to nuclear weapons while managing the risks".\]